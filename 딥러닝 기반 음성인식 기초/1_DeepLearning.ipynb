{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2P95_7d0FaA"
   },
   "source": [
    "reference : https://seungheondoh.netlify.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9ZzN28Fx9nP"
   },
   "source": [
    "### Deep Learning\n",
    "\n",
    "딥러닝은 모형들의 output이 다시 input으로 들어가서 학습을 이어나가는 Layer 구조를 가지는 모형입니다. 모형의 특징을 결정하는데는 다음과 같은 요소들이 필요하게 됩니다. 앞으로는 밑의 개념들을 하나씩 정리해 보려고합니다.\n",
    "\n",
    "- 모델 Building\n",
    "    - Connectivity patterns\n",
    "    - Nonlinearity Modules\n",
    "    - Loss function\n",
    "- 모델 학습\n",
    "    - Optimization\n",
    "    - Hyper Parameters\n",
    "\n",
    "### Connectivity Pattern\n",
    "\n",
    "딥러닝은 여러 레이어를 쌓아나가는 구조입니다. 뉴런은 각 레이어에 있으며, 레이어들간의 연결관계에 따라서 패턴이 나눠집니다.\n",
    "- Fully-Conntected\n",
    "- Convolutional\n",
    "- Dilated\n",
    "- Recurrent\n",
    "- Skip / Residual\n",
    "- Random\n",
    "\n",
    "이 Colab문서에서는 간단한 Connectivity Pattern을 정리하고자 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLUEhKbuyakY"
   },
   "source": [
    "### 1. FC Layer\n",
    "\n",
    "뉴럴넷의 가장 간단한 아키텍쳐를 세팅해보려고 합니다. 저희에게 필요한 것은 레이어의 갯수와 뉴런의 갯수 그리고, 각 layer들의 연결 패턴을 고려해야합니다. 이번 케이스에서는 Input, hidden, output layer를 각각 하나씩 가지고 있는 모듈을 만들어 보려고합니다. connectivity pattern은 fully connected 로 하겠습니다. \n",
    "\n",
    "- Layer number\n",
    "- Neuron number\n",
    "- connectivity pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WqHZauc-ympd"
   },
   "outputs": [],
   "source": [
    "layer_dims = [5,4,3,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z93D98Aix_Jz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qk4-OHiyW2r"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    # dictionary 객체 생성\n",
    "    parameters = {}\n",
    "    # 총 layer들의 길이를 계산\n",
    "    L = len(layer_dims)\n",
    "    # 레이어들을 돌면서, 레이어들 간의 weight와 bias의 초기값의 난수 생성\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        # assert를 통해, dimension을 맞추줍니다. 틀릴시 error 발생\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "VIYHJCtZyhB0",
    "outputId": "53411ed6-fb6f-45c9-ba7a-c1b9929ba659"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.17694975, -0.03220242, -0.17241916,  0.29330886, -0.1066996 ],\n",
       "        [ 0.26427396, -0.31122566, -0.02103218,  0.07796209, -0.09580865],\n",
       "        [ 0.86182432,  0.26511277, -0.88483861, -0.22328476, -0.15744259],\n",
       "        [-0.0084656 , -0.51556544,  0.02093199,  0.56761057, -0.47629799]]),\n",
       " 'W2': array([[ 0.47707234, -0.1527564 ,  0.52509004,  0.39958705],\n",
       "        [ 0.25217565,  0.26663892,  0.18941114, -0.0093512 ],\n",
       "        [-0.39912022, -0.24331496,  0.01081425, -0.71950724]]),\n",
       " 'W3': array([[ 1.36670928, -0.89458938,  0.22871491]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep(layer_dims)\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVLOrDctyiYJ"
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    # W에 A를 내적하게 됩니다. 그후에는 b를 더해줍니다.\n",
    "    Z = W.dot(A) + b\n",
    "    # Z의 shape이 input과 weight의 shape과 동일한지를 체크합니다.\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    # 계산단계에서 사용한 값을 cache에 저장해둡니다.\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    # Activation function의 종류에 따라서 값을 나누어 줍니다.\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    # Shape이 input과 weight와 동일한지 체크해줍니다.\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    # linear 연산과 activation 연산을 cache에 저장해둡니다.\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7l2lEn8yqLA"
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    # cache 들의 list입니다.\n",
    "    caches = []\n",
    "    A = X\n",
    "    # weight와 bias가 저장되어 있기 때문에 //2 를 해주어야 layer의 사이즈가 됩니다.\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # hidden layersms relu를 통과\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # output layer는 sigmoid를 통과하게 한다\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFtZQbOmyuXn"
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(5,4)\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "AL, caches = L_model_forward(X, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "OLdwxzoQyvYU",
    "outputId": "1a7363f7-017f-4435-ec56-63e389fd177b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57649229, 0.58858761, 0.50625841, 0.67020139]])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fw94JTO8yzVx"
   },
   "source": [
    "### 4. Cost Function\n",
    "\n",
    "우리는 신경망을 통과한 $\\hat{y}$값을 찾을 수 있었습니다. 하지만 우리의 실제 y 레이블과는 다른 값일 가능성이 매우 크기 떄문에 이를 반영하여 학습을 시켜야합니다. Cost function은 여러가지 종류가 있습니다만, 이번의 경우에는 cross-entropy 함수를 사용하려고합니다. 이후에 Cost function에 대해서도 정리해보도록 하겠습니다.\n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
    "\n",
    "\n",
    "__Arguments__\n",
    "\n",
    "AL : 뉴럴넷을 통과해서 나오게된 $\\hat{y}$ 입니다. shape (1, number of examples)\n",
    "Y -- 실제 \"label\" vector 입니다. (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "__Returns__\n",
    "\n",
    "cost : cross-entropy cost\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ixcYFMCCywnn"
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1.0/m)*np.sum(np.multiply(Y,np.log(AL)) + np.multiply(1-Y, np.log(1-AL)))\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qqYUH-KNy0-s",
    "outputId": "30ce2c70-f641-4c99-c8cf-5df03026ea6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.7947985413241794\n"
     ]
    }
   ],
   "source": [
    "cost = compute_cost(AL, Y)\n",
    "print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PlO9Ax3zB__"
   },
   "source": [
    "__Backpropagation Process__\n",
    "\n",
    "Backpropagation은 다음과 같은 process를 가지게 됩니다.\n",
    "\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward\n",
    "- Layer -> Layer backward\n",
    "\n",
    "### Linear backward\n",
    "\n",
    "Linear 한 영역에서 backward 과정은 다음과 같은 인자를 받게 됩니다.\n",
    "\n",
    "__Arguments__\n",
    "\n",
    "dZ : Z의 변화량입니다. linear 부분에서 ouput이 cost function 에 대한 gradient를 나타냅니다.\n",
    "\n",
    "cache : forward과정에서 필요한 값을 받아옵니다. tuple 형태의 (A_prev, W, b) 값들을 받아옵니다.\n",
    "\n",
    "__Returns__\n",
    "\n",
    "dA_prev : Linear 구간의 input으로 들어왔었던, 지난 레이어의 activation 을 통과한 A가 cost function에 대한 변화량입니다.\n",
    "\n",
    "dW : Linear 구간의 weight의 cost function에 대한 변화량 입니다.\n",
    "\n",
    "db : Linear 구간의 bias의 cost function 에 대한 변화량 입니다.\n",
    "\n",
    "\n",
    "### Linear-Activation backward\n",
    "\n",
    "Activation function $g(.)$ 에 대해서 Linear-activate backward는 다음과 같이 계산됩니다.\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$.  \n",
    "\n",
    "__Arguments__\n",
    "\n",
    "dA : 현재 layer의 gradient값이 인자로 들어옵니다.\n",
    "\n",
    "cache : forward pass에서 계산했던 linear(Z) 부분과 activation(A) 부분의 계산값들을 받습니다.\n",
    "\n",
    "__Returns__\n",
    "\n",
    "dA_prev : Linear 구간의 input으로 들어왔었던, 지난 레이어의 activation 을 통과한 A가 cost function에 대한 변화량입니다.\n",
    "\n",
    "dW : Linear 구간의 weight의 cost function에 대한 변화량 입니다.\n",
    "\n",
    "db : Linear 구간의 bias의 cost function 에 대한 변화량 입니다.\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nOXjh6Jiy2SE"
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3bnf_h4zFd7"
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ,cache[0].T)/m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db =  linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TpycP9ofzGek"
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {} # 빈 dictionary 호출\n",
    "    L = len(caches) # 레이어의 갯수를 caches로 부터 받아옵니다.\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # Shape을 AL과 동일하게 해줍니다.\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y,AL)- np.divide(1-Y, 1-AL))\n",
    "    # caches index를 잡아둡니다.\n",
    "    current_cache = caches[L-1] \n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # indexing입니다.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+1)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "6my6eMaXzTb6",
    "outputId": "cacf60ed-ce2e-42d1-f185-a71b8672ee81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dA0': array([[ 0.22615928,  0.05056479,  0.03725571,  0.26606754],\n",
       "        [-0.08893561,  0.06033779,  0.00678003,  0.08820048],\n",
       "        [-0.31533332, -0.00864554,  0.03630182, -0.37436958],\n",
       "        [ 0.18297927, -0.11513727, -0.06175443,  0.00178919],\n",
       "        [-0.22823877,  0.09102261,  0.02246496, -0.08833891]]),\n",
       " 'dA1': array([[ 0.24583103, -0.17543675, -0.21054402,  0.28579099],\n",
       "        [-0.25786842,  0.1840272 ,  0.22085354, -0.29978506],\n",
       "        [ 0.31603321, -0.22553637, -0.27066926,  0.36740456],\n",
       "        [ 0.31965621, -0.22812192, -0.27377221,  0.37161649]]),\n",
       " 'dA2': array([[ 0.78789736, -0.56228112, -0.67480121,  0.91597046],\n",
       "        [-0.51572388,  0.36804515,  0.44169598, -0.59955504],\n",
       "        [ 0.13185238, -0.09409615, -0.11292606,  0.15328505]]),\n",
       " 'dW1': array([[ 0.01718012,  0.07410485, -0.07740157, -0.01062174, -0.08069423],\n",
       "        [ 0.00538101, -0.10324898,  0.0618198 ,  0.01496212,  0.01577951],\n",
       "        [-0.03120275,  0.06252994, -0.13021064, -0.04486433, -0.07889032],\n",
       "        [ 0.00794814,  0.15945834, -0.08395172,  0.04680436, -0.01708135]]),\n",
       " 'dW2': array([[ 0.04299149, -0.09749935,  0.31746886, -0.12159809],\n",
       "        [-0.02814039,  0.0638189 , -0.20780153,  0.0795929 ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]]),\n",
       " 'dW3': array([[0.11232412, 0.03374054, 0.        ]]),\n",
       " 'db1': array([[0.0802695 ],\n",
       "        [0.0460068 ],\n",
       "        [0.17085944],\n",
       "        [0.02288357]]),\n",
       " 'db2': array([[ 0.11669637],\n",
       "        [-0.07638445],\n",
       "        [ 0.        ]]),\n",
       " 'db3': array([[0.08538493]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = L_model_backward(AL, Y, caches)\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zkh4mDVzNV6"
   },
   "source": [
    "### 6. Update parameter\n",
    "\n",
    "파라미터를 업데이트 하는 규칙은 생각보다 간편합니다. Learning rate인 $\\alpha$ 에 Gradient를 곱해서 현재의 parameter에 빼주면 새로운 parameter가 됩니다.\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
    "\n",
    "__Arguments__\n",
    "\n",
    "parameters : 파라미터들이 담겨져 있는 parameter dictionary입니다.\n",
    "grads : Gradient들이 담겨있는 입니다\n",
    "\n",
    "__Returns__\n",
    "\n",
    "parameters : 업데이트되어있는 파라미터들이 담긴 dictionary입니다\n",
    "- parameters[\"W\" + str(l)] = ... \n",
    "- parameters[\"b\" + str(l)] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJ-ZAky-zKrx"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # 레이어의 갯수입니다.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\"+str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "R4vLw4-8zO1V",
    "outputId": "c9f9c86e-e8df-42e5-8df5-1c2b5d1cad8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.17780875, -0.03590766, -0.16854908,  0.29383995, -0.10266489],\n",
       "        [ 0.26400491, -0.30606321, -0.02412317,  0.07721398, -0.09659762],\n",
       "        [ 0.86338446,  0.26198627, -0.87832808, -0.22104154, -0.15349807],\n",
       "        [-0.00886301, -0.52353836,  0.02512958,  0.56527036, -0.47544392]]),\n",
       " 'W2': array([[ 0.47492277, -0.14788143,  0.5092166 ,  0.40566695],\n",
       "        [ 0.25358267,  0.26344798,  0.19980122, -0.01333085],\n",
       "        [-0.39912022, -0.24331496,  0.01081425, -0.71950724]]),\n",
       " 'W3': array([[ 1.36109307, -0.8962764 ,  0.22871491]]),\n",
       " 'b1': array([[-0.00401348],\n",
       "        [-0.00230034],\n",
       "        [-0.00854297],\n",
       "        [-0.00114418]]),\n",
       " 'b2': array([[-0.00583482],\n",
       "        [ 0.00381922],\n",
       "        [ 0.        ]]),\n",
       " 'b3': array([[-0.00426925]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = update_parameters(parameters, grads, 0.05)\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhjxRAV2zfJt"
   },
   "source": [
    "### CNN 구조\n",
    "\n",
    "CNN 은 이미지의 __특징을 추출__ 하는 부분과 __클래스를 분류__ 하는 부분으로 나눌 수 있습니다. \n",
    "<br>\n",
    "\n",
    "__특징 추출__ : Convolution Layer,Pooling Layer\n",
    "\n",
    "입력 데이터를 필터가 순회하며 합성곱을 계산하고, 그 계산 결과를 이용하여 Feature map을 만듭니다. Feature map는 sub-sampled 를 통해서 차원을 줄여주는 효과를 가지게 됩니다. Convolution Layer는 Filter 크기, Stride, Padding 적용여부, Max Pooling의 크기에 따라서 출력 데이터의 Shape이 결정됩니다. \n",
    "\n",
    "- Convolution Layer : 입력데이터에 필터(Filter or Weight)를 적용 후 활성함수를 반영하는 요소입니다.\n",
    "- Pooling Layer(Subsampling) : spatial 차원의 다운샘플링을 책임집니다.\n",
    "\n",
    "__클래스 분류__ : Fully Connected Layer\n",
    "\n",
    "### Convolve Window\n",
    "\n",
    "이번에는 Filter를 이동시키며 convolution 연산하는 과정을 구해보려고 합니다. input의 volume을 받아서(3차원), 모든 position의 input에 filter를 적용해보고자합니다. Convolution 연산은 element wise multiplication으로 이루어집니다.\n",
    "    \n",
    "__Argument__\n",
    "\n",
    "- a_slice_prev : Filter가 적용될 Input입니다. (f, f, n_C_prev)\n",
    "- W : Filter의 사이즈입니다. (f, f, n_C_prev)\n",
    "- b : Bais입니다. - matrix of shape (1, 1, 1)\n",
    "    \n",
    "__Returns__\n",
    "\n",
    "- Z : Convolution 연산의 결과로 나오는 값입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "S8DhM3pSzQML",
    "outputId": "bb255e58-b5ff-4153-82d7-4dc0d4e5392e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3, 3, 2) (4, 7, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "def zero_pad(X,pad):\n",
    "    X_pad = np.pad(X,((0,0),(pad,pad),(pad,pad),(0,0)),'constant',constant_values=0)\n",
    "    return X_pad\n",
    "\n",
    "x = np.random.randn(4,3,3,2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print(x.shape, x_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fK3cWERizozF"
   },
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    # Element-wise product\n",
    "    s = a_slice_prev * W\n",
    "    # 채널을 기반으로 모두 더해줍니다.\n",
    "    Z = np.sum(s)\n",
    "    # Bias b를 더해줍니다.\n",
    "    Z = Z + np.float(b)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGVlTaZ4zyQW"
   },
   "source": [
    "### 3. Convolutional Neural Networks - Forward pass\n",
    "\n",
    "Forward pass에서는 다양한 필터를 통해서, 구현을 위해 2D input의 horizental과 vertial index를 계산하면서 filter를 적용해보려고합니다. stack이 되는 output을 계산해 보려고합니다.\n",
    "\n",
    "Convolution의 output shape을 결정하는 식은 다음과 같습니다.\n",
    "\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 \n",
    "$$\n",
    "\n",
    "$$n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "\n",
    "$$n_C = \\text{number of filters used in the convolution}$$\n",
    "\n",
    "\n",
    "__Arguments__\n",
    "\n",
    "- A_prev : Input으로 들어가는 Matrix입니다. 데이터의 batch m, Hight, Width, Channel이 포함되어 있습니다. (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "- W : Weights, Filter입니다. (f, f, n_C_prev, n_C)\n",
    "- b : Biases (1, 1, 1, n_C)\n",
    "- hparameters : \"stride\" 와 \"pad\"를 결정하는 python dictionary입니다.\n",
    "\n",
    "__Returns__\n",
    "\n",
    "- Z : conv output입니다. (m, n_H, n_W, n_C)\n",
    "- cache : conv_backward() 에 도움을 줄 cache입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KkTP79VzztU6"
   },
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    # Input의 shpae을 정의합니다.\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    # filter의 shape을 정의합니다.\n",
    "    (f,f,n_C_prev,n_C) = W.shape\n",
    "    # input dictionary에서 받을 value 값입니다.\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Conv의 output volumn을 정의해줍니다\n",
    "    n_H = int(((n_H_prev - f + (2*pad)) / stride)+1)\n",
    "    n_W = int(((n_W_prev - f + (2*pad)) / stride)+1)\n",
    "    \n",
    "    # output volumn을 initialize해줍시다\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Padding을 설정해줍니다.\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "    \n",
    "    for i in range(m): #batch에 있는 traindata를 조회\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        for h in range(n_H): #hight를 돌고\n",
    "            for w in range(n_W): #width를 돌고\n",
    "                for c in range(n_C): #Channel을 돌면서\n",
    "                    #input의 slice를 해줍시다\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:]\n",
    "                    Z[i,h,w,c]=conv_single_step(a_slice_prev,W[...,c],b[...,c])\n",
    "    assert(Z.shape == (m,n_H,n_W,n_C))\n",
    "    cache = (A_prev,W,b,hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "usMOujT7z2Cz"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "eqm6UF9Uz3iB",
    "outputId": "b22c28a4-2d21-41b1-eb85-2647a94af6fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = 0.048995203528855794\n"
     ]
    }
   ],
   "source": [
    "print(\"Z's mean =\", np.mean(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZL7f2CHIz7r_"
   },
   "source": [
    "### 4. Forward Pooling\n",
    "\n",
    "Activation function은 잠시 넘어가고 이제 MAX-POOL과 AVG-POOL을 구현해 봅시다.\n",
    "\n",
    "패딩이 없다고 가정하고 Pooling을 구현해보려고합니다. 다음과 같은 공식을 기반으로 구현이 이루어집니다.\n",
    "\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_C = n_{C_{prev}}$$\n",
    "\n",
    "__Arguments__\n",
    "- A_prev : Input데이터입니다. 일반적으로 Convolution layer의 결과값이 됩니다. (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "- hparameters : \"f\" 와 \"stride\"가 담긴 dictionary입니다.\n",
    "- mode : \"max\" or \"average\"를 결정하는 인자입니다.\n",
    "\n",
    "__Returns__\n",
    "- A : pool layer의 output입니다. (m, n_H, n_W, n_C)\n",
    "- cache : Backward pass를 계산히기 위해 저장해두는 캐시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "asmSeTN2z4nZ"
   },
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode=\"max\"):\n",
    "    # input의 shape을 받아옵니다\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    # filter size와 stride size를 받아옵니다.\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    # ouput dimension을 잡아줍시다\n",
    "    n_H = int(1+(n_H_prev-f)/stride)\n",
    "    n_W = int(1+(n_W_prev-f)/stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        A[i,h,w,c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i,h,w,c] = np.mean(a_prev_slice)\n",
    "                        \n",
    "    cache = (A_prev, hparameters)\n",
    "    assert(A.shape ==(m,n_H,n_W,n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "yGbGzrSwz9AD",
    "outputId": "ba255de0-fab9-4f69-f4cc-de724dd85a67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.74481176 0.86540763 1.13376944]]]\n",
      "\n",
      "\n",
      " [[[1.13162939 1.51981682 2.18557541]]]]\n",
      "[[[[ 0.02105773 -0.20328806 -0.40389855]]]\n",
      "\n",
      "\n",
      " [[[-0.22154621  0.51716526  0.48155844]]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(A)\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsIHOWQXcXve"
   },
   "source": [
    "RNN Forward Pass\n",
    "\n",
    "실제 RNN 모델의 경우에는 단일한 time-step이 적용되는 경우는 거의 없습니다. 이번에는 RNN cell이 10개가 붙어있다고 생각해 봅시다.\n",
    "\n",
    "__Arguments__<br>\n",
    "- x -- 모든 time-step의 input 데이터입니다. shape은 (n_x, m, T_x)결정됩니다. \n",
    "- a0 -- 초기 hidden state입니다. hidden state의 갯수와, 데이터의 갯수로 shape이 결정됩니다.(n_a, m) \n",
    "- parameters -- python dictionary로 다음과 같은 정보가들어옵니다.\n",
    "    - Waa -- hidden state에 대한 Weight matrix입니다.(n_a, n_a)\n",
    "    - Wax -- input에 대한 Weight matrix입니다.(n_a, n_x)\n",
    "    - Wya -- hidden state에서 output으로 가는 Weight matrix 입니다. (n_y, n_a)\n",
    "    - ba -- hidden state에 대한 Bias입니다. (n_a, 1)\n",
    "    - by -- output에 대한 Bias입니다 (n_y, 1)\n",
    "\n",
    "__Returns__<br>\n",
    "- a -- 모든 time step에 대한 hidden state 백터입니다. (n_a, m, T_x)\n",
    "- y_pred -- 예측된 Output입니다. (n_y, m, T_x)\n",
    "- caches -- Backprop에 필요한 Caches입니다. (list of caches, x)\n",
    "\n",
    "__Task__<br>\n",
    "1. $a$인 hidden state vector의 공간을 zero vector로 만들어 줍니다.\n",
    "2. $a_0$ (initial hidden state)을 초기화 합니다.\n",
    "3. Time step을 기반으로 for loop를 통해서 RNN cell 을 돌려줍니다. :\n",
    "    - $a$ ($t^{th}$ position)를 계산합니다. 즉 이전 스탭에서 현재 스탭으로 업데이트하는 것이죠.\n",
    "    - $a$ ($t^{th}$ position)를 캐시에 저장해 줍니다.\n",
    "    - $y_pred$를 다시 업데이트 해줍니다.\n",
    "    - 캐시를 저장합니다.\n",
    "4. 마지막 step의 $a$, $y$와 caches를 저장해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioOiyd4uck-j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rRKdT-h0chhF"
   },
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \n",
    "    # parameter의 dict에서 데이터를 호출합니다.\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # 1. hidden state를 구현해봅시다.\n",
    "    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
    "    # 2. Predict Output를 구현해봅니다.\n",
    "    yt_pred = softmax(np.dot(Wya, a_next)+by)\n",
    "    # 3. Cache에 저장합시다.\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tZXYqkLz-IO"
   },
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, parameters):\n",
    "    # caches라는 cache를 저장할 list를 선언합니다.\n",
    "    caches = []\n",
    "    \n",
    "    # Dimension을 맞추기 위해 input sequence 가준으로 unpacking 해줍니다.\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    # a와 y를 초기화 합니다.\n",
    "    a = np.zeros((n_a,m,T_x))\n",
    "    y_pred = np.zeros((n_y,m,T_x))\n",
    "    \n",
    "    # a_next를 초기화 합니다.\n",
    "    a_next = a0\n",
    "    \n",
    "    # time step을 돌면서 rnn cell을 작동 시킵니다.\n",
    "    for t in range(T_x):\n",
    "        # 1. hidden step을 계산해 줍니다.\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)\n",
    "        # 2. 새로운 hidden step을 a에 반영해 줍니다.\n",
    "        a[:,:,t] = a_next\n",
    "        # y의 값 역시 업데이트 해줍니다.\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        \n",
    "        # 결과값을 저장해 줍니다.\n",
    "        caches.append(cache)\n",
    "        \n",
    "    caches = (caches, x)\n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDBPJ0u1ccgZ"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a, y_pred, caches = rnn_forward(x, a0, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "colab_type": "code",
    "id": "qEAteSwNcd1g",
    "outputId": "d2ef72e0-be02-4319-e068-8e74ffeb7b45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.94679727,  0.99998902,  0.99859532,  0.99998339],\n",
       "        [ 0.52566384,  0.99993012, -0.99996484,  0.99999942],\n",
       "        [ 0.84483137,  0.99971338,  0.63006186,  0.99999504],\n",
       "        [ 0.95268814,  0.9993585 ,  0.87825787,  0.9999979 ],\n",
       "        [ 0.99996124,  0.9999908 ,  0.99897665,  0.6007902 ],\n",
       "        [ 0.94354992, -0.99516219,  0.99987056,  0.99902443],\n",
       "        [-0.31399689,  0.90494133,  0.99964112,  0.999997  ],\n",
       "        [ 0.9985362 , -0.95921363,  0.97076661,  0.99792727],\n",
       "        [ 0.99995626,  0.99994879,  0.55718656,  0.97797982],\n",
       "        [ 0.99981346, -0.99139889, -0.90908533,  0.99994617]],\n",
       "\n",
       "       [[ 0.9017533 , -0.0035545 , -0.40146936,  0.47240999],\n",
       "        [-0.64008899, -0.99808521,  0.90937915,  0.99308063],\n",
       "        [-0.61107796, -0.93987579, -0.82797531, -0.99944897],\n",
       "        [ 0.69254271,  0.70004749,  0.95560602,  0.03494921],\n",
       "        [ 0.99323355,  0.98511719,  0.93041097,  0.99371087],\n",
       "        [-0.97376282,  0.89291419,  0.9777595 ,  0.68670555],\n",
       "        [ 0.96905989,  0.84821902,  0.99428756,  0.91339115],\n",
       "        [ 0.63387486, -0.0561147 , -0.06557296, -0.0515541 ],\n",
       "        [ 0.34250436,  0.76229843,  0.89552076, -0.60056774],\n",
       "        [-0.83173001,  0.94604565,  0.99882607,  0.98957886]],\n",
       "\n",
       "       [[ 0.97087407,  0.96868192,  0.9860278 ,  0.62437977],\n",
       "        [ 0.32586574, -0.91931824,  0.62536152,  0.83109594],\n",
       "        [ 0.94009082,  0.79972708,  0.98280633, -0.9114071 ],\n",
       "        [ 0.97102425,  0.69671275,  0.99918672, -0.81446397],\n",
       "        [ 0.99869819,  0.81461615, -0.34958752,  0.98390801],\n",
       "        [-0.9227938 ,  0.99784354,  0.99857354,  0.94312789],\n",
       "        [-0.97880697,  0.62394864,  0.99397484, -0.99894842],\n",
       "        [-0.78819779,  0.19186314,  0.91860743,  0.9916753 ],\n",
       "        [ 0.99957809, -0.91253018,  0.71732866, -0.45986869],\n",
       "        [-0.84758466, -0.98924985,  0.99999082,  0.99746386]],\n",
       "\n",
       "       [[-0.9104497 ,  0.99927595,  0.94217573, -0.98743686],\n",
       "        [-0.96081056,  0.99726769, -0.98947737, -0.97175622],\n",
       "        [-0.93837279, -0.99812032, -0.99997534,  0.9759714 ],\n",
       "        [ 0.9957971 ,  0.98744174, -0.91907333,  0.30870646],\n",
       "        [ 0.84483456,  0.05888194,  0.57284256, -0.99798536],\n",
       "        [ 0.98777081, -0.99999738, -0.91229958, -0.77235035],\n",
       "        [-0.73832733,  0.84553649, -0.98818114,  0.08833992],\n",
       "        [-0.99876665,  0.81798993,  0.99999724,  0.73642847],\n",
       "        [ 0.41236695,  0.75086186, -0.36929754,  0.99998852],\n",
       "        [ 0.93310421, -0.01108915, -0.99769046, -0.94005036]],\n",
       "\n",
       "       [[-0.99935897, -0.57882882,  0.99953622,  0.99692362],\n",
       "        [-0.99999375,  0.77911235, -0.99861469, -0.99833267],\n",
       "        [ 0.98895163,  0.9905525 ,  0.87805502,  0.99623046],\n",
       "        [ 0.9999802 ,  0.99693738,  0.99745184,  0.97406138],\n",
       "        [-0.9912801 ,  0.98087418,  0.76076959,  0.54482277],\n",
       "        [ 0.74865774, -0.59005528, -0.97721203,  0.92063859],\n",
       "        [-0.96279238, -0.99825059,  0.95668547, -0.76146336],\n",
       "        [-0.99251598, -0.95934467, -0.97402324,  0.99861032],\n",
       "        [ 0.93272501,  0.81262652,  0.65510908,  0.69252916],\n",
       "        [-0.1343305 , -0.99995298, -0.9994704 , -0.98612292]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9PPg_8vceBz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "1.DeepLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
